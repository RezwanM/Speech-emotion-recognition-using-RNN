{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Libraries</h1>\n",
    "\n",
    "At first, let's import all the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa as rosa\n",
    "import os\n",
    "import tensorflow.keras as keras\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Counting Audio Frames</h1>\n",
    "\n",
    "Next, we will count the number of audio frames for each audio file in our dataset and calculate the median number of frames. This is done so that we have the same size for each feature vector. Don't worry, it will make sense later! We will extract the RMS energy from each frame to keep track of the number of frames from all the audio files.\n",
    "\n",
    "RAVDESS files were recorded at a sampling frequency of 48 kHz. A higher sampling rate gives a better audio resolution, but it also means that we will need to store more data. That's why we will resample the audio files at 16 kHz, which is good enough for most cases. Also, we will use a frame length of 512 samples (i.e. 32 ms) and a hop length of 256 samples (i.e. 16 ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save directory path of RAVDESS in 'folder_path'.\n",
    "folder_path = 'C:/Users/rezwa/Downloads/RAVDESS'\n",
    "\n",
    "# Create a list of directories inside the RAVDESS directory.\n",
    "folder_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list for storing the number of audio frames in each file.\n",
    "num_frames = []\n",
    "\n",
    "# Loop for calculating averge number of frames for the dataset.\n",
    "for foldername in folder_list:\n",
    "    file_path = folder_path + '/' + foldername\n",
    "    file_list = os.listdir(file_path)\n",
    "    for filename in file_list:\n",
    "        # Read WAV file. 'rosa.core.load' returns sampling frequency in 'fs' and audio signal in 'sig'.\n",
    "        sig, fs = rosa.core.load(file_path + '/' + filename, sr=16000)\n",
    "        \n",
    "        # 'rosa.feature.rms' extracts rms energies from audio frames (one per frame) and stores them into 'rms_feat'.\n",
    "        rms_feat = rosa.feature.rms(y=sig, frame_length=512, hop_length=256)\n",
    "        num_frames.append(rms_feat.shape[1])\n",
    "    \n",
    "    # Go one level up in the directory tree.\n",
    "    os.chdir('..')\n",
    "    \n",
    "# Calculate the Median of the number of frames for all audio files. This will then be used to cap the maximum number of frames per audio file, which in turn will be used as the number of RNN units.\n",
    "median_num_frames = statistics.median(num_frames)\n",
    "\n",
    "# Convert float to integer.\n",
    "median_num_frames = int(median_num_frames)\n",
    "print(median_num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Extracting Features</h1>\n",
    "\n",
    "Now that we know how many audio frames we will process for each audio file, we can extract the features and save them to a Pandas dataframe. We will extract 26 MFCCs per frame, 7 spectral contrasts per frame, 2 polynomial coefficients per frame, and 1 RMS energy per frame.\n",
    "\n",
    "If an audio file has lower number of audio frames than our median number of frames (the cap), we will pad the audio with zeros to match its length to the median. On the other hand, if an audio file has more audio frames than the median number of frames, we will remove the excess frames from that audio file to match the lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a dummy Numpy array (row vector).\n",
    "result_array = np.empty([1, (36*median_num_frames)+1])\n",
    "\n",
    "# Declare a variable to be later used in reshaping the feature array.\n",
    "i = 0\n",
    "\n",
    "# Loop for feature extraction.\n",
    "for foldername in folder_list:\n",
    "    file_path = folder_path + '/' + foldername\n",
    "    file_list = os.listdir(file_path)\n",
    "    for filename in file_list:\n",
    "        # Read WAV file. 'rosa.core.load' returns sampling frequency in 'fs' and audio signal in 'sig'.\n",
    "        sig, fs = rosa.core.load(file_path + '/' + filename, sr=16000)\n",
    "        \n",
    "        # 'rosa.feature.mfcc' extracts n_mfccs from signal and stores it into 'mfcc_feat'\n",
    "        mfcc_feat = rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26, n_fft=512, hop_length=256, htk=True)\n",
    "        spec_feat = rosa.feature.spectral_contrast(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "        poly_feat = rosa.feature.poly_features(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "        rms_feat = rosa.feature.rms(y=sig, frame_length=512, hop_length=256)\n",
    "\n",
    "        # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "        feat0 = np.append(mfcc_feat, spec_feat, axis=0)\n",
    "        feat1 = np.append(feat0, poly_feat, axis=0)\n",
    "        feat2 = np.append(feat1, rms_feat, axis=0)\n",
    "\n",
    "        # Transpose the array to flip the rows and columns. This is done so that the features become column parameters, making each row an audio frame.\n",
    "        transp_feat = feat2.T\n",
    "\n",
    "        # Note: The 'cap frame number' is basically the limit we set for the number of frames for each audio file, so that all audio files have equal lengths when processing.\n",
    "        \n",
    "        if transp_feat.shape[0] < median_num_frames:\n",
    "            # If number of frames is smaller than the cap frame number, we pad the array in order to reach our desired dimensions.\n",
    "            # Pad the array so that it matches the cap frame number. The second value in the argument contains two tuples which indicate which way to pad how much.  \n",
    "            transp_feat = np.pad(transp_feat, ((0, median_num_frames-transp_feat.shape[0]), (0,0)), constant_values=0)\n",
    "\n",
    "        elif transp_feat.shape[0] > median_num_frames:\n",
    "            # If number of frames is larger than the cap frame number, we delete rows (frames) which exceed the cap frame number in order to reach our desired dimensions.\n",
    "            # Define a tuple which contains the range of the row indices to delete.\n",
    "            row_del_index = (range(median_num_frames, transp_feat.shape[0], 1))\n",
    "            transp_feat = np.delete(transp_feat, row_del_index, axis=0)\n",
    "\n",
    "        else:\n",
    "            # If number of frames match the cap frame length, perfect!\n",
    "            transp_feat = transp_feat\n",
    "\n",
    "        # Transpose again to flip the rows and columns. This is done so that the features become row parameters, making each column an audio frame.\n",
    "        transp2_feat = transp_feat.T\n",
    "\n",
    "        # Flatten the entire 2D Numpy array into 1D Numpy array. So, the first 36 values of the 1D array represent the features for first frame, the second 36 represent the features for second frame, and so on till the final (cap) frame.\n",
    "        # 'C' means row-major ordered flattening.\n",
    "        feat_flatten = transp2_feat.flatten('C')\n",
    "\n",
    "        # Save emotion label from file name.\n",
    "        label = os.path.splitext(os.path.basename(file_path + '/' + filename))[0].split('-')[2]\n",
    "\n",
    "        # Create a new Numpy array 'sample' to store features along with label.\n",
    "        sample = np.insert(feat_flatten, obj=36*median_num_frames, values=label)\n",
    "\n",
    "        result_array = np.append(result_array, sample)\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    # Go one level up in the directory tree.\n",
    "    os.chdir('..')\n",
    "    \n",
    "\n",
    "# Convert 1D Numpy array to 2D array. Argument must be a Tuple. i+1 because we have i audio files plus a dummy row.\n",
    "result_array = np.reshape(result_array, (i+1,-1))\n",
    "\n",
    "# Delete first dummy row from 2D array.\n",
    "result_array = np.delete(result_array, 0, 0)\n",
    "\n",
    "# Save the feature array into a Pandas dataframe.\n",
    "df = pd.DataFrame(data=result_array)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving Features to CSV</h1>\n",
    "\n",
    "We can now save the Pandas dataframe containing all the features as a CSV file. For this project, we will only use data for seven emotions - happy, sad, anger, fear, surprise, and neutral. Also, we will replace the integer labels that we got from the file names with string labels for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label only the last (target) column.\n",
    "df = df.rename({36*median_num_frames: \"Emotion\"}, axis='columns')\n",
    "# Delete calm emotion data.\n",
    "df.drop(df[df['Emotion'] == 2.0].index, inplace = True)\n",
    "# Rename integer labels with string labels.\n",
    "df['Emotion'].replace({1.0: \"Neutral\", 3.0: \"Happy\", 4.0: \"Sad\", 5.0: \"Angry\", 6.0: \"Fearful\", 7.0: \"Disgust\", 8.0: \"Surprised\"}, inplace=True)\n",
    "# Reset row (audio files) indexing.\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Save as CSV file.\n",
    "df.to_csv(\"C:/Users/rezwa/Documents/RAVDESS_Librosa_RNN.csv\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
