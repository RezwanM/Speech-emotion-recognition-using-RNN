{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing the Libraries</h1>\n",
    "\n",
    "At first, let's import all the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as rosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sounddevice as sd\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing the Trained Model</h1>\n",
    "\n",
    "Next, we will import the RNN that we trained for this project, along with the saved mean and standard deviation arrays for the input features from training. The model files should be present in the same directory as the 'Training_testing.ipynb' file. It is convenient to run this notebook in that same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RNN model from the h5 file \n",
    "rnn_h5 = tf.keras.models.load_model('RNN_RAVDESS.h5')\n",
    "\n",
    "# Load the arrays containing means and standard deviations of features from training for the RNN model\n",
    "mean_X = np.load('mean_X.npy')\n",
    "std_X = np.load('std_X.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Setting the Audio Parameters</h1>\n",
    "\n",
    "Our goal is to record around 7 seconds of audio continuously and make predictions on the recordings using our RNN model. We will use a sampling rate of 16 kHz with 512 samples per frame. For 7.36 seconds of audio, this gives us a frame size of 230 frames per recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 16000  # Record at 16000 samples per second\n",
    "median_num_frames = 230  # From training data\n",
    "seconds = 7.36  # Length of recording (230*512/16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Defining the String Labels</h1>\n",
    "\n",
    "In order to display the final output, we need to print out the string labels so that the reader can see the name of the emotion being predicted by the machine. Here, we will define a function that will switch the labels for us. The dictionary called 'switcher' maps the input integer label to the corresponding output string label. Additionally, we will print out the corresponding emoji of the predicted emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(argument):\n",
    "    switcher = {\n",
    "        1:\"Neutral\",\n",
    "        2:\"Happy\",\n",
    "        3:\"Sad\",\n",
    "        4:\"Angry\",\n",
    "        5:\"Fearful\",\n",
    "        6:\"Disgust\",\n",
    "        7:\"Surprised\"\n",
    "    }\n",
    "    return switcher.get(argument, \"Nothing\")\n",
    "\n",
    "def label_emoji(argument):\n",
    "    switcher = {\n",
    "        1:emoji.emojize(\":neutral_face:\"),\n",
    "        2:emoji.emojize(\":grinning_face_with_smiling_eyes:\"),\n",
    "        3:emoji.emojize(\":disappointed_face:\"),\n",
    "        4:emoji.emojize(\":angry_face:\"),\n",
    "        5:emoji.emojize(\":fearful_face:\"),\n",
    "        6:emoji.emojize(\":face_vomiting:\"),\n",
    "        7:emoji.emojize(\":hushed_face:\"),\n",
    "    }\n",
    "    return switcher.get(argument, \"Nothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Recording Audio and making Predictions</h1>\n",
    "\n",
    "This is the cell which will perform the actual audio recordings! We will continuously record in 7.36-second intervals. The length is chosen to match the average length of the RAVDESS recordings. Our voice recordings will then be fed to our RNN model, which will then print out the predicted emotion labels. Make sure you have a working microphone in your system. You can either use the default microphone on your machine, or plug in an external microphone using a USB or a 3.5 mm audio jack. For making the output more presentable, we will also print out the corresponding emojis associated with each emotion!\n",
    "\n",
    "Notice that the while loop runs indefinitely. You can stop the code from running by pressing the stop button on the top panel of your Jupyter Notebook, or by simply pressing the appropriate shortcut keys for your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recording...')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # In sounddevice, frames mean samples!\n",
    "        # Blocksize is the number of samples per frame!\n",
    "\n",
    "        # Store recorded signal into a Numpy array.\n",
    "        sig = sd.rec(frames=int(fs*seconds), samplerate=fs, channels=1, blocksize=512)\n",
    "\n",
    "        sd.wait() # Wait until recording is finished\n",
    "\n",
    "        sig = np.reshape(sig, (117760,))    # 16000 Hz * 7.36 seconds\n",
    "\n",
    "\n",
    "        # RNN feature extraction\n",
    "        # 'rosa.feature.mfcc' extracts n_mfccs from signal and stores it into 'mfcc_feat'.\n",
    "        mfcc_feat = rosa.feature.mfcc(y=sig, sr=fs, n_mfcc=26, n_fft=512, hop_length=256, htk=True)\n",
    "\n",
    "        spec_feat = rosa.feature.spectral_contrast(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "\n",
    "        poly_feat = rosa.feature.poly_features(y=sig, sr=fs, n_fft=512, hop_length=256)\n",
    "\n",
    "        rms_feat = rosa.feature.rms(y=sig, frame_length=512, hop_length=256)\n",
    "\n",
    "        # Append the three 1D arrays into a single 1D array called 'feat'.\n",
    "        feat0 = np.append(mfcc_feat, spec_feat, axis=0)\n",
    "\n",
    "        feat1 = np.append(feat0, poly_feat, axis=0)\n",
    "\n",
    "        feat2 = np.append(feat1, rms_feat, axis=0)\n",
    "        \n",
    "        # Transpose the array to flip the rows and columns. This is done so that the features become column parameters, making each row an audio frame.\n",
    "        transp_feat = feat2.T\n",
    "\n",
    "        # Note: The 'cap frame number' is basically the limit we set for the number of frames for each audio file, so that all audio files have equal lengths when processing.\n",
    "        \n",
    "        if transp_feat.shape[0] < median_num_frames:\n",
    "            # If number of frames is smaller than the cap frame number, we pad the array in order to reach our desired dimensions.\n",
    "            # Pad the array so that it matches the cap frame number. The second value in the argument contains two tuples which indicate which way to pad how much.  \n",
    "            transp_feat = np.pad(transp_feat, ((0, median_num_frames-transp_feat.shape[0]), (0,0)), constant_values=0)\n",
    "\n",
    "        elif transp_feat.shape[0] > median_num_frames:\n",
    "            # If number of frames is larger than the cap frame number, we delete rows (frames) which exceed the cap frame number in order to reach our desired dimensions.\n",
    "            # Define a tuple which contains the range of the row indices to delete.\n",
    "            row_del_index = (range(median_num_frames, transp_feat.shape[0], 1))\n",
    "            transp_feat = np.delete(transp_feat, row_del_index, axis=0)\n",
    "\n",
    "        else:\n",
    "            # If number of frames match the cap frame length, perfect!\n",
    "            transp_feat = transp_feat\n",
    "\n",
    "        # Transpose again to flip the rows and columns. This is done so that the features become row parameters, making each column an audio frame.\n",
    "        transp2_feat = transp_feat.T\n",
    "\n",
    "        # Flatten the entire 2D Numpy array into 1D Numpy array. So, the first 36 values of the 1D array represent the features for first frame, the second 36 represent the features for second frame, and so on till the final (cap) frame.\n",
    "        # 'C' means row-major ordered flattening.\n",
    "        feat_rnn = transp2_feat.flatten('C')\n",
    "\n",
    "        feat_rnn = np.reshape(feat_rnn, (1,-1)) \n",
    "\n",
    "        # Standardize the inputs means and standard deviations of features from training for RNN model.\n",
    "        feat_centered_rnn = (feat_rnn - mean_X)/std_X\n",
    "\n",
    "        # Reshaping feat_centered to 3D Numpy array for feeding into the RNN. RNNs require 3D array input.\n",
    "        # 3D dimensions are (layers, rows, columns).\n",
    "        feat_3D = np.reshape(feat_centered_rnn, (feat_centered_rnn.shape[0], median_num_frames, 36))\n",
    "\n",
    "        # Transpose tensors so that rows=features and columns=frames.\n",
    "        feat_3D_posed = tf.transpose(feat_3D, perm=[0, 2, 1])\n",
    "\n",
    "        # Make prediction using RNN model.\n",
    "        pred = rnn_h5.predict(feat_3D_posed)\n",
    "\n",
    "        # Convert One Hot label to integer label.\n",
    "        pred = int(np.argmax(pred, axis=1))\n",
    "        \n",
    "        # Get the corresponding string label.\n",
    "        emotion = change_label(pred)\n",
    "        \n",
    "        # Get the corresponding emoji.\n",
    "        smiley = label_emoji(pred)\n",
    "        \n",
    "        # Print the output.\n",
    "        print(smiley, \" : \", emotion)\n",
    "        \n",
    "        del sig\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('Recording has ended!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
